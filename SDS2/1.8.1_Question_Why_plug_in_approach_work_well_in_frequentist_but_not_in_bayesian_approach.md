That's a very insightful question that highlights a key difference between the philosophies and goals of Bayesian and frequentist inference, especially concerning prediction.
You are right that the simple **plug-in approach** (calculating a point estimate $\hat{\theta}$ and using it as if it were the true value, e.g., predicting via $f(y_{new}|\hat{\theta})$) is often viewed as **less satisfactory within the Bayesian framework** compared to using the full posterior predictive distribution. But it **is a very common approach in frequentist statistics**.
Here's why the perspectives differ:
**Why Plug-in is Less Ideal in Bayesian Inference:**
1. **Parameter Uncertainty is Central:** The core output of a Bayesian analysis is the **posterior distribution $\pi(\theta|y_{obs})$**. This distribution explicitly quantifies our *remaining uncertainty* about the parameter $\theta$ after observing the data.
2. **Ignoring Uncertainty:** Using only a single point estimate $\hat{\theta}$ (whether it's the posterior mean, median, or mode) fundamentally ignores all the other plausible values of $\theta$ indicated by the posterior distribution. It essentially pretends we know $\theta$ exactly, discarding the uncertainty information that Bayesian inference works hard to provide.
3. **Underestimating Predictive Uncertainty:** As discussed (and shown on slides 50, 81-83), predictions depend on two sources of randomness: the inherent variability of the process given $\theta$ (from $f(y_{new}|\theta)$) AND the uncertainty about $\theta$ itself (from $\pi(\theta|y_{obs})$). The plug-in approach only captures the first source. The **Bayesian posterior predictive distribution** $m(y_{new}|y_{obs}) = \int f(y_{new}|\theta)\pi(\theta|y_{obs})d\theta$ correctly incorporates *both* sources by averaging over the posterior uncertainty in $\theta$. This usually results in wider, more realistic predictive intervals.
**Why Plug-in is Common/Acceptable in Frequentist Inference:**
1. **Parameters as Fixed Constants:** In the classical frequentist view, the true parameter $\theta$ is considered a **fixed, unknown constant**. There is no probability distribution assigned to $\theta$ representing belief or uncertainty about its value. Uncertainty lies in the data arising from random sampling and, consequently, in the estimators derived from the data.
2. **Focus on Good Estimators:** A primary goal in frequentist statistics is to find point estimators $\hat{\theta}$ (like the Maximum Likelihood Estimator, MLE) that have good long-run properties (consistency, unbiasedness, efficiency) over hypothetical repeated samples.
3. **Natural Prediction Strategy:** Given that $\theta$ is treated as fixed and the goal is often to find the "best" single estimate $\hat{\theta}$, the most straightforward way to make a prediction is to use this best estimate within the model: $f(y_{new}|\hat{\theta})$. There's no posterior distribution $\pi(\theta|y_{obs})$ to average over.
4. **Accounting for Estimation Error:** Frequentist prediction intervals *can* account for the uncertainty *in the estimator* $\hat{\theta}$ (i.e., how much $\hat{\theta}$ might vary from sample to sample), but this is handled differently than integrating over a posterior distribution for $\theta$.
5. **Asymptotic Properties:** In many situations, especially with large sample sizes ($n$), frequentist estimators $\hat{\theta}$ become very precise (i.e., the uncertainty about the true $\theta$ captured by the sampling distribution of $\hat{\theta}$ becomes small). In such cases, the difference between the plug-in prediction $f(y_{new}|\hat{\theta})$ and what might be predicted if $\theta$ were known becomes negligible, providing an asymptotic justification for the plug-in approach.
**In essence:** The Bayesian approach explicitly models uncertainty about $\theta$ using a probability distribution, making averaging over this distribution the natural way to predict, while the frequentist approach treats $\theta$ as fixed, making plugging in the best estimate $\hat{\theta}$ the natural course. The Bayesian approach argues that plug-in ignores crucial uncertainty information provided by the posterior, while the frequentist approach relies on finding good estimators and often uses large-sample theory where plug-in approximations work well.