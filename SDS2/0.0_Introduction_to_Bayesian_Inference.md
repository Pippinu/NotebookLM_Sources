# Introduction to the Basics of Bayesian Inference
Think of statistics as a way to learn about the world from data. There are different philosophies or approaches to doing this. One major approach is called "Frequentist" statistics (which you might have encountered briefly if you've seen things like `p-values` or confidence intervals). Bayesian inference is another major approach.
The core idea behind Bayesian inference is **updating your beliefs based on evidence (data)**.
Here's a breakdown of the key concepts:
1. **Probability as Belief:** In the Bayesian view, probability isn't just about the long-run frequency of events. It's also used to represent our **degree of belief** or uncertainty about something unknown. This "unknown" could be anything â€“ the true average height of people in a city, whether a coin is fair, or the effectiveness of a new drug.
2. **Prior Belief (Prior Distribution):** Before we look at any data, we usually have some initial belief about the unknown quantity. This might be very vague ("I have no idea") or more specific ("I think it's probably around this value"). In Bayesian terms, this initial belief is represented mathematically by a **prior probability distribution**, often denoted as $\pi(\theta)$. Here, $\theta$ (theta) represents the unknown quantity we're interested in (like the average height or the coin's fairness). (See slides page 9, source 14).
3. **Likelihood:** This represents how well different possible values of the unknown quantity $\theta$ explain the data we actually observed. It's based on our statistical model, which describes the probability of observing the data *given* a specific value for $\theta$. This is often written as $f(y|\theta)$ or $p(y|\theta)$, where 'y' is our observed data. It tells us, "If the true value was $\theta$, how likely would it be to see the data 'y'?" (See slides page 9, source 14).
4. **Posterior Belief (Posterior Distribution):** This is the goal! After observing the data, we update our prior beliefs. The result is the **posterior probability distribution**, written as $\pi(\theta|y)$. It represents our updated beliefs about the unknown quantity $\theta$ *after* considering the evidence from the data 'y'. It essentially combines what we initially believed (the prior) with what the data tells us (the likelihood). (See slides page 13, source 24).
5. **Bayes' Rule (or Bayes' Theorem):** This is the mathematical engine that combines the prior and the likelihood to get the posterior. The basic formula looks like this (See slides page 11, source 21; page 12, source 23):
```latex
\pi(\theta|y) = \frac{f(y|\theta) \pi(\theta)}{m(y)}
```
* $\pi(\theta|y)$ is the **Posterior** (what we want to find).
* $f(y|\theta)$ is the **Likelihood** (how well $\theta$ explains the data y).
* $\pi(\theta)$ is the **Prior** (our initial belief about $\theta$).
* $m(y)$ is the **Marginal Likelihood** or **Evidence**. It's the probability of observing the data 'y' averaged over all possible values of $\theta$. It acts as a normalizing constant to ensure the posterior is a proper probability distribution (integrates/sums to 1). (See slides page 14, source 25). Often, we just write:
```latex
\pi(\theta|y) \propto f(y|\theta) \pi(\theta)
```
Which means "the posterior is proportional to the likelihood times the prior". We figure out the normalizing constant $m(y)$ later if needed. (See slides page 15, source 26).
**In simple terms:** Bayesian inference is about starting with some initial beliefs (prior), collecting data, and using the data (likelihood) via Bayes' Rule to update those beliefs into a more informed state (posterior). The posterior distribution contains all our knowledge about the unknown quantity after seeing the data.