Okay, let's delve into the **Conjugate Analysis for the Normal Model**, based on slides 106 through 134. \
The Normal distribution is fundamental in statistics, and understanding its Bayesian analysis is key. \
The slides cover two main conjugate scenarios:

**Scenario 1: Normal Data with KNOWN Variance $\sigma^2$ (Slides 107-119)**

This is a somewhat artificial case, as the **variance is rarely known exactly in practice**, but it's a crucial stepping stone.

1.  **Model Setup:**
    * **Likelihood:** You have $n$ i.i.d. observations $Y_1, ..., Y_n$ assumed to come from $N(\theta, \sigma^2)$, where the mean $\theta$ is **unknown**, but the variance $\sigma^2$ is **known**.
    * The likelihood function for $\theta$ is derived from the joint density *(likelihood kernel with respect to Î¸ when $y_1, ..., y_n$ are fixed)*:
        $$L_y(\theta) \propto \exp \left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 \right\}$$
        Through algebraic manipulation (completing the square in $\theta$ or recognizing the sufficient statistic $\bar{y}_n$), the kernel simplifies to a form involving $$\exp \{ -\frac{1}{2} a \theta^2 + b \theta \}$$ for some $a, b$ that depend on $y_1, ..., y_n$ and the known $\sigma^2$. Specifically, $a = n/\sigma^2$ and $b = n\bar{y}_n/\sigma^2$. (Math shown on slides 108-111).
    * **How to derive Likelihood Function:** 

        1.  **Individual Densities:** In Scenario 1, we assume each $Y_i$ is drawn independently from a *univariate* Normal distribution $N(\theta, \sigma^2)$. The density for a single $Y_i$ is:
            $$f(y_i|\theta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{1}{2\sigma^2}(y_i-\theta)^2\}$$
        2.  **Joint Density of Independent Univariates:** Because the $Y_i$'s are assumed independent and identically distributed (i.i.d.), their joint probability density is the product of their individual densities:
            $$f(y_1, ..., y_n | \theta, \sigma^2) = \prod_{i=1}^n f(y_i|\theta, \sigma^2)$$
            $$f(y_1, ..., y_n | \theta, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\theta)^2\right\}$$
        3.  **Likelihood Function:** When we consider this joint density as a function of the unknown parameter $\theta$ (for fixed data $y_1, ..., y_n$), we get the likelihood function $L_y(\theta)$. \
        For Bayesian inference, we often only care about the parts that depend on $\theta$, so we look at the kernel:
            $$
            L_y(\theta) \propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\theta)^2\right\}
            $$


2.  **Conjugate Prior:**
    * Since the likelihood kernel is exponential in a quadratic function of $\theta$, the conjugate prior for $\theta$ is also a **Normal distribution**.

        **Why the Normal prior is conjugate for the Normal likelihood (with known variance)?** :
        1.  **Matching Forms:** Both the Normal likelihood kernel (for $\theta$, with known variance) and the Normal prior kernel have the same mathematical structure: they are proportional to the exponential of a *quadratic function* of the parameter $\theta$.  
            Let's write this form as:  
            $$
            e^{\text{Quadratic}(\theta)}
            $$
        
        2.  **Multiplication Property:** When calculating the posterior kernel (Likelihood $\times$ Prior), you multiply these exponential forms:  
            $$
            e^{\text{Quadratic}_L(\theta)} \times e^{\text{Quadratic}_P(\theta)} = e^{\text{Quadratic}_L(\theta) + \text{Quadratic}_P(\theta)}
            $$

        3.  **Closure Property:** The sum of two quadratic functions is still a quadratic function.

        4.  **Result:** Therefore, the posterior kernel is also proportional to the exponential of a quadratic function of $\theta$. This specific mathematical form *is* the kernel of a Normal distribution.

        5.  **Conclusion:** Because a Normal prior combined with this Normal likelihood yields a posterior that is also Normal, the Normal distribution family is conjugate for this model.

    * We specify the prior as $\theta \sim N(\mu_0, \tau_0^2)$, where $\mu_0$ is the prior mean and $\tau_0^2$ is the prior variance. Its kernel is $\propto \exp \left\{ -\frac{1}{2\tau_0^2} (\theta - \mu_0)^2 \right\}$.

3.  **Posterior Distribution:**
    * Multiplying the likelihood kernel and the prior kernel results in another exponential of a quadratic function of $\theta$. \
    This confirms the posterior distribution for $\theta$ is also Normal.
    $$\pi(\theta|y) \propto L_y(\theta) \pi(\theta) \propto \exp \{ -\frac{1}{2} a_{post} \theta^2 + b_{post} \theta \}$$
    * The posterior distribution is $\theta | y \sim N(\mu_y, \tau_y^2)$.

4.  **Posterior Parameter Updates & Interpretation (Slide 115):**
    * The posterior parameters are found by combining the information from the prior and the likelihood:
        * **Posterior Mean:** 
        <span style="font-size: 120%;">
            $$\mu_y = \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar{y}_n}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
            $$
        </span>
        
        * **Posterior Variance:** 
            <span style="font-size: 120%;"> $$\tau_y^2 = \left( \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} \right)^{-1}$$ </span>
    * **Precision Interpretation (Slides 116-119):** The formulas become particularly intuitive when using *precision* (inverse variance: $\nu = 1/\tau^2$). Let $\psi = 1/\sigma^2$ be the known data precision and $\nu_0 = 1/\tau_0^2$ be the prior precision. Then:
        * **Posterior Precision:** 
        <span style="font-size: 120%;">$$\nu_y = \nu_0 + n\psi$$</span>
        *Precisions add up!* The total precision is the prior precision plus the precision contributed by the $n$ data points.
        * **Posterior Mean:** 
        <span style="font-size: 120%;">$$\mu_y = w \mu_0 + (1-w) \bar{y}_n$$</span>
        where the weight <span style="font-size: 150%;">$w = \frac{\nu_0}{\nu_0 + n\psi}$</span>. \
        The posterior mean is a **weighted average** of the prior mean ($\mu_0$) and the sample mean ($\bar{y}_n$), weighted by their relative precisions (prior precision $\nu_0$ vs. total data precision $n\psi$).

Okay, here is the explanation for Scenario 2 (Normal data with unknown mean and variance) reformatted to match the style you used for Scenario 1:

**Scenario 2: Normal Data with UNKNOWN Mean $\theta$ and UNKNOWN Variance $\sigma^2$ (Slides 120-134)**

This is the more common and realistic scenario.

1.  **Model Setup:**
    * **Likelihood:** $Y_i | (\theta, \sigma^2) \sim N(\theta, \sigma^2)$ (i.i.d.), where both $\theta$ and $\sigma^2$ are unknown. \
    The likelihood function now **depends on both parameters**:
        $$L_y(\theta, \sigma^2) \propto (\sigma^2)^{-n/2} \exp \left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 \right\}$$
    * This can be rewritten using sufficient statistics $\bar{y}_n$ and sample variance $s^2 = \frac{1}{n-1}\sum(y_i - \bar{y}_n)^2$:
        $$L_y(\theta, \sigma^2) \propto (\sigma^2)^{-n/2} \exp \left\{ -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\theta - \bar{y}_n)^2] \right\}$$
        (See Slide 131).

2.  **Conjugate Prior (Normal-Inverse Gamma):**
    * A conjugate prior exists for the *pair* $(\theta, \sigma^2)$, but it requires a specific **hierarchical structure** (Slide 120, Slide 125):
        * First, specify a prior for the variance:
            $$\sigma^2 \sim InvGamma(\text{shape} = \nu_0/2, \text{scale} = \nu_0\sigma^2_0/2)$$
            (This is equivalent to the precision $\lambda=1/\sigma^2$ having a $Gamma(\nu_0/2, \text{rate} = \nu_0\sigma^2_0/2)$ distribution). The hyperparameters $\nu_0 > 0$ and $\sigma^2_0 > 0$ control this prior.
        * Then, specify a prior for the mean *conditional* on the variance:
            $$\theta | \sigma^2 \sim N(\mu_0, \sigma^2/\kappa_0)$$
            The hyperparameters are $\mu_0$ and $\kappa_0 > 0$. Note that the prior variance for $\theta$ scales with $\sigma^2$.
    * This joint prior distribution is called the **Normal-Inverse Gamma (N-IG)** distribution and is defined by the four hyperparameters (Slide 122) $$(\mu_0, \kappa_0, \nu_0, \sigma^2_0)$$
    * *(Parameterization Note: Be mindful of whether Gamma/Inverse Gamma parameters are specified as rate or scale. The slides use a scale convention for InvGamma related to $\nu_0\sigma_0^2/2$.)* (Slides 121, 205-207).

3.  **Marginal Prior for $\theta$ (Slide 123, 126):**
    * If you integrate $\sigma^2$ out of the joint N-IG prior, the marginal prior distribution for $\theta$ alone is a **scaled and shifted Student's t-distribution** with $\nu_0$ degrees of freedom, mean $\mu_0$, and scale $\sqrt{\sigma^2_0/\kappa_0}$. (Definition of Student's t on slides 112-113, 124).

4.  **Posterior Distribution (Slide 127):**
    * The joint posterior distribution $\pi(\theta, \sigma^2 | y)$ retains the **same Normal-Inverse Gamma structure** as the prior.
        $$\sigma^2 | y \sim InvGamma(\nu_n/2, \nu_n\sigma^2_n/2)$$
        $$\theta | \sigma^2, y \sim N(\mu_n, \sigma^2/\kappa_n)$$

5.  **Posterior Parameter Updates (Slide 127):**
    * The four hyperparameters are updated based on the prior hyperparameters and the data summaries ($\bar{y}_n, s^2, n$):
        * $$\kappa_n = \kappa_0 + n$$
        * $$\nu_n = \nu_0 + n$$
        * $$\mu_n = \frac{\kappa_0\mu_0 + n\bar{y}_n}{\kappa_n}$$
            (Still a weighted average of prior mean and sample mean)
        * $$\nu_n\sigma^2_n = \nu_0\sigma^2_0 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_n}(\bar{y}_n - \mu_0)^2$$
            (This combines prior sum of squares, data sum of squares, and a term for the difference between data mean and prior mean).
    * *(Mathematical details for deriving these updates are outlined in slides 129-134).*

6.  **Marginal Posterior for $\theta$ (Slide 128):**
    * Similar to the marginal prior, the marginal posterior distribution for $\theta$ alone (after integrating out $\sigma^2$) is also a **scaled and shifted Student's t-distribution**, but now with $\nu_n$ degrees of freedom, centered at $\mu_n$, and with scale $\sqrt{\sigma^2_n/\kappa_n}$.
    * This is often the distribution used for inference (credible intervals, tests) about the mean $\theta$ when the variance is unknown.

In summary, the Normal model allows for conjugate analysis in both the known and unknown variance cases. The unknown variance case requires a joint Normal-Inverse Gamma prior, leading to a Student's t marginal distribution for the mean parameter $\theta$, both in the prior and the posterior. The update rules, while more complex for the N-IG case, follow interpretable patterns of combining prior information with data summaries.