Let's discuss the predictive distributions specifically for the **Poisson-Gamma conjugate model**, based on slides 75 through 83.
Remember the setup:
* Likelihood: $Y_i | \theta \sim Poisson(\theta)$, for $i=1, ..., n$ (i.i.d.)
* Prior: $\theta \sim Gamma(shape=s_{prior}, rate=r_{prior})$
* Posterior: $\theta | y \sim Gamma(shape=s_{post}, rate=r_{post})$, where $s_{post} = s_{prior} + \sum y_i$ and $r_{post} = r_{prior} + n$.
**1. Prior Predictive Distribution (Slides 75-78)**
* **Goal:** Find the distribution of a single potential observation $Y$ *before* seeing any data ($y_1, ..., y_n$). This is $m(y) = \int f(y|\theta)\pi(\theta)d\theta$.
* **Derivation:** We need to integrate the Poisson PMF multiplied by the Gamma PDF over all possible $\theta > 0$:
$$m(y) = \int_0^\infty \left( \frac{e^{-\theta} \theta^y}{y!} \right) \left( \frac{r_{prior}^{s_{prior}}}{\Gamma(s_{prior})} \theta^{s_{prior}-1} e^{-r_{prior}\theta} \right) d\theta$$
Rearranging terms and using properties of the Gamma integral, this simplifies significantly.
* **Result:** The resulting distribution for $Y$ is the **Negative Binomial distribution**[cite: 215].
* The slides show that this distribution can be parameterized in terms of $s = s_{prior}$ and $p = \frac{r_{prior}}{r_{prior} + 1}$[cite: 212].
* Specifically, the probability mass function is:
$$P(Y=y) = \binom{s_{prior} + y - 1}{y} p^{s_{prior}} (1-p)^y$$
(where $p = \frac{r_{prior}}{r_{prior} + 1}$ and $1-p = \frac{1}{r_{prior} + 1}$)[cite: 215].
* **Interpretation:** For integer $s_{prior}$, this represents the probability of observing $y$ "failures" (e.g., non-events if we think of Poisson) before the $s_{prior}$-th "success" in a sequence of Bernoulli trials with success probability $p$[cite: 213, 217].
* **Mean & Variance:** The mean is $E[Y] = \frac{s_{prior}(1-p)}{p} = \frac{s_{prior}}{r_{prior}} = E[\theta]$ (as expected by law of total expectation). The variance is $Var[Y] = \frac{s_{prior}(1-p)}{p^2} = \frac{s_{prior}(r_{prior}+1)}{r_{prior}^2} = E[\theta] + \frac{E[\theta]}{r_{prior}} = E[Var[Y|\theta]] + Var[E[Y|\theta]]$. Note that $Var[Y] > E[Y]$, showing overdispersion compared to a simple Poisson[cite: 216, 219].
**2. Posterior Predictive Distribution (Slides 79-80, 83)**
* **Goal:** Find the distribution of a *new* observation $Y_{new}$ *after* seeing the data $y = (y_1, ..., y_n)$. This is $m(y_{new}|y) = \int f(y_{new}|\theta)\pi(\theta|y)d\theta$.
* **Derivation:** The logic is identical to the prior predictive, but now we integrate the Poisson likelihood $f(y_{new}|\theta)$ against the *posterior* distribution $\pi(\theta|y)$, which is $Gamma(s_{post}, r_{post})$.
* **Result:** The posterior predictive distribution for $Y_{new}$ is also **Negative Binomial**, but with parameters derived from the posterior Gamma distribution[cite: 220]:
* The shape parameter is $s = s_{post} = s_{prior} + \sum y_i$.
* The probability parameter is $p_{post} = \frac{r_{post}}{r_{post} + 1} = \frac{r_{prior} + n}{r_{prior} + n + 1}$.
* **Mean & Variance:** The mean is $E[Y_{new}|y] = \frac{s_{post}}{r_{post}} = \frac{s_{prior} + \sum y_i}{r_{prior} + n} = E[\theta|y]$ (the posterior mean of $\theta$). The variance is $Var[Y_{new}|y] = \frac{s_{post}(r_{post}+1)}{r_{post}^2} = E[Y_{new}|y] \times \frac{r_{prior}+n+1}{r_{prior}+n}$[cite: 220].
**3. Comparison with Plug-in Prediction (Slides 81-82)**
* **Key Point:** The Bayesian predictive distributions (Negative Binomial) fully account for the uncertainty about the rate parameter $\theta$ (as described by the Gamma prior or posterior).
* **Plug-in:** A simpler but potentially misleading approach is to calculate a point estimate for $\theta$ (like the posterior mean $\hat{\theta} = E[\theta|y]$) and then predict $Y_{new}$ using the likelihood directly: $Y_{new} \sim Poisson(\hat{\theta})$.
* **Difference:** The plug-in Poisson prediction ignores the remaining uncertainty about $\theta$. The proper Bayesian Negative Binomial prediction incorporates this uncertainty, resulting in a distribution with **higher variance** ($Var[Y_{new}|y] > E[Y_{new}|y]$ for the Negative Binomial) compared to the plug-in Poisson prediction (where variance equals mean, $Var[Y_{new}|\hat{\theta}] = \hat{\theta}$)[cite: 223]. This extra variability in the Bayesian prediction is generally more realistic.
In summary, for the Poisson-Gamma conjugate model, both the prior and posterior predictive distributions for a single count observation follow Negative Binomial distributions, reflecting the uncertainty about the underlying Poisson rate $\theta$.