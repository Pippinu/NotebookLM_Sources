**Graphical Models, Dependency, and Exchangeability (Slides 89-105)**
    * **Graphs & DAGs (Directed Acyclic Graphs):** Introduces the formalism of using graphs (nodes and edges) to represent the structure of joint probability distributions (Slides 89-92).
        * Defines key terminology (parent, child, ancestor, descendant, etc.) (Slide 91).
        * Focuses on DAGs, showing how the structure of a DAG corresponds directly to a specific factorization of the joint probability distribution: $p(\text{all variables}) = \prod p(\text{variable}_k | \text{parents of variable}_k)$ (Slides 93-94, 97 [cite: 222]).
        * Explains how DAGs visually represent conditional independence relationships (e.g., a node is conditionally independent of its non-descendants given its parents) (Slide 95, Slide 100).
        * Briefly touches on limitations and interpretation nuances (Slide 98).
    * **Exchangeability & De Finetti's Theorem:**
        * Defines an "exchangeable" sequence of random variables (where the joint probability doesn't depend on the order) (Slide 102 [cite: 230]).
        * Presents De Finetti's representation theorem: This fundamental theorem states that if a sequence of observations is (infinitely) exchangeable, its joint distribution can be represented *as if* the observations were independent and identically distributed (i.i.d.) conditional on some parameter $\theta$ (or a random probability measure P), where $\theta$ (or P) itself is drawn from a prior distribution $\pi(\cdot)$ (Slides 103-105).
        * This provides a strong subjectivist justification for using hierarchical models of the form $\theta \sim \pi(\theta)$ and $Y_i | \theta \sim f(y_i|\theta)$ (i.i.d.) commonly used in Bayesian analysis.