**Conjugate Priors for the Univariate Exponential Family** (Slides 85-88).
This section provides a more general and theoretical view of why certain prior distributions "work well" (are conjugate) with certain likelihoods. It turns out many common distributions belong to a larger class called the exponential family, and this family has a very neat structure regarding conjugate priors.
**1. The Univariate Exponential Family (Slide 85, source 160)**
A family of probability distributions (parameterized by $\theta$) belongs to the one-parameter exponential family if its probability density function (pdf) or probability mass function (pmf) can be written in the form:
$f(x|\theta) = h(x) \exp \{ \eta(\theta) T(x) - B(\theta) \}$
Let's break down the components:
* $h(x)$: A function depending only on the data $x$ (the "underlying measure").
* $T(x)$: The **sufficient statistic**. All the information in the data $x$ about the parameter $\theta$ is captured through $T(x)$.
* $\eta(\theta)$: A function of the parameter $\theta$.
* $B(\theta)$: A function of the parameter $\theta$, often called the log-normalizer or log-partition function, because $e^{B(\theta)}$ is what ensures the distribution integrates/sums to 1.
It's often more convenient to use the **natural parameterization**. Let $\psi = \eta(\theta)$ be the **natural parameter**. Then the form becomes:
$f(x|\psi) = c(\psi) h(x) \exp \{ \psi T(x) \}$
Here, $c(\psi) = e^{-A(\psi)}$ (where $A(\psi)$ corresponds to $B(\theta)$) is the normalizing constant that depends only on the natural parameter $\psi$. It's defined such that $\int c(\psi) h(x) e^{\psi T(x)} dx = 1$ (or sum equals 1 for discrete cases).
*Examples:* Binomial, Poisson, Gamma, Normal (with known variance), Exponential, Beta distributions are all members of this family. For each, you can identify the specific forms of $h(x), T(x), \psi$, and $c(\psi)$.
**2. The General Conjugate Prior (Slide 86, source 161)**
The key insight is that if the likelihood has the form $f(x|\psi) \propto c(\psi) e^{\psi T(x)}$, then a prior distribution for the **natural parameter $\psi$** that "matches" this form should also involve powers of $c(\psi)$ and $e^{\psi}$.
The general form of the **conjugate prior density** for the natural parameter $\psi$ is given as:
$\pi_{conj}(\psi | n_0, t_0) \propto c(\psi)^{n_0} \exp\{n_0 t_0 \psi\}$
This prior distribution is defined by two **hyperparameters**:
* $n_0 > 0$
* $t_0$ (which must be in the range of possible values for the sufficient statistic $T(x)$).
**3. The Posterior Update Rule (Slide 87, source 163)**
If you use this conjugate prior $\pi_{conj}(\psi | n_0, t_0)$ and observe $n$ i.i.d. data points $x_1, ..., x_n$, the posterior distribution for $\psi$ remains in the same family!
The likelihood for $n$ i.i.d. observations is:
$L(\psi | x_1, ..., x_n) = \prod_{i=1}^{n} [c(\psi) h(x_i) \exp \{ \psi T(x_i) \}] \propto [c(\psi)]^n \exp\{ \psi \sum_{i=1}^{n} T(x_i) \}$
The posterior is $\pi(\psi | \text{data}) \propto L(\psi | \text{data}) \times \pi_{conj}(\psi | n_0, t_0)$:
$\pi(\psi | \text{data}) \propto [c(\psi)]^n \exp\{ \psi \sum T(x_i) \} \times [c(\psi)]^{n_0} \exp\{ n_0 t_0 \psi \}$
$\pi(\psi | \text{data}) \propto [c(\psi)]^{n_0 + n} \exp\{ \psi (n_0 t_0 + \sum T(x_i)) \}$
$\pi(\psi | \text{data}) \propto [c(\psi)]^{n_0 + n} \exp\{ (n_0 + n) \left( \frac{n_0 t_0 + \sum T(x_i)}{n_0 + n} \right) \psi \}$
This is exactly the form of the conjugate prior, but with updated hyperparameters! The posterior is proportional to $\pi_{conj}(\psi | n_{post}, t_{post})$ where:
* $n_{post} = n_0 + n$
* $t_{post} = \frac{n_0 t_0 + \sum_{i=1}^{n} T(x_i)}{n_0 + n} = \frac{n_0 t_0 + n \bar{T}_n}{n_0 + n}$ (where $\bar{T}_n$ is the sample mean of the sufficient statistic).
**4. Interpretation of Hyperparameters (Slide 88, source 165)**
This general form provides a nice interpretation for the hyperparameters:
* $n_0$: Can be interpreted as the **prior strength** or the number of **"pseudo-observations"** represented by the prior. It dictates how much weight the prior carries relative to the data ($n$).
* $t_0$: Can be interpreted as the **prior guess for the average value of the sufficient statistic**, $E[T(X)]$.
The update rule for $t_{post}$ clearly shows the posterior location $t_{post}$ as a **weighted average** of the prior location $t_0$ and the data's average sufficient statistic $\bar{T}_n$, weighted by the prior strength $n_0$ and the data size $n$:
$t_{post} = \left(\frac{n_0}{n_0+n}\right) t_0 + \left(\frac{n}{n_0+n}\right) \bar{T}_n$
This structure directly mirrors the weighted average updates we saw for the posterior means in the Binomial-Beta and Poisson-Gamma examples, because those models are specific instances of this general framework.
In essence, the exponential family provides a unifying mathematical structure that explains why simple conjugate priors exist for many common statistical models and why their update rules often take the form of adding counts/sample sizes and forming weighted averages.