Okay, let's talk about **Examples of Non-Conjugate Models**. This is the next logical step after covering the standard conjugate cases, as highlighted in the syllabus and starting around slide 144 [cite: 318] in your lecture notes.

**What is a Non-Conjugate Model?**

Simply put, a Bayesian model is non-conjugate if the chosen prior distribution $\pi(\theta)$ and the likelihood function $L_y(\theta)$ are such that the resulting posterior distribution $\pi(\theta|y) \propto L_y(\theta)\pi(\theta)$ **does not belong to the same standard, recognizable family of distributions as the prior**.

In conjugate analysis, we carefully picked priors (like Beta for Binomial, Gamma for Poisson, Normal-Inverse Gamma for Normal) specifically because they *did* lead to posteriors in the same family, allowing for simple analytical updates. When this "matching" property doesn't hold, the model is non-conjugate.

**Example from the Slides: Normal Model with Independent Priors (Slides 144-145)**

The slides use the Normal data model ($Y_i | (\theta, \sigma^2) \sim N(\theta, \sigma^2)$) as an example, but instead of the hierarchical Normal-Inverse Gamma *conjugate* prior, they consider specifying *independent* priors for the mean $\theta$ and the variance $\sigma^2$ (or precision $\lambda = 1/\sigma^2$).

1.  **Likelihood:** Remains the same as before:
    $L_y(\theta, \sigma^2) \propto (\sigma^2)^{-n/2} \exp \left\{ -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\theta - \bar{y}_n)^2] \right\}$ [cite: 289]
2.  **Independent Priors (Non-Conjugate Choice):** A seemingly natural, but non-conjugate, choice is to assume $\theta$ and $\sigma^2$ are independent *a priori* and assign them separate priors[cite: 318]:
    * $\theta \sim N(\mu_0, \tau_0^2)$ (Normal prior for the mean)
    * $\sigma^2 \sim InvGamma(\nu_0/2, \nu_0\sigma_0^2/2)$ (Inverse Gamma prior for the variance)
    * So, the joint prior is $\pi(\theta, \sigma^2) = \pi(\theta)\pi(\sigma^2)$.

3.  **Posterior Distribution:** The joint posterior is proportional to likelihood times joint prior:
    $$\pi(\theta, \sigma^2 | y) \propto L_y(\theta, \sigma^2) \times \pi(\theta) \times \pi(\sigma^2)$$   $$\pi(\theta, \sigma^2 | y) \propto \left[ (\sigma^2)^{-n/2} e^{ -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\theta - \bar{y}_n)^2] } \right] \times \left[ e^{ -\frac{1}{2\tau_0^2}(\theta-\mu_0)^2 } \right] \times \left[ (\sigma^2)^{-(\nu_0/2+1)} e^{ -\frac{\nu_0\sigma_0^2/2}{\sigma^2} } \right]$$
    While you can algebraically combine the terms involving $\theta$ and $\sigma^2$, the resulting functional form **does not correspond to any standard bivariate distribution** like the Normal-Inverse Gamma distribution we saw in the conjugate case[cite: 319]. The prior independence is "broken" by the likelihood term which links $\theta$ and $\sigma^2$.

**Why is this important?**

Because the posterior distribution doesn't have a standard form, we usually **cannot** calculate posterior summaries (like means, medians, credible intervals) or the normalizing constant analytically.

This is the crucial point where **approximation methods become essential**. Non-conjugate models necessitate techniques like:

* Numerical Integration (often difficult in high dimensions)
* Monte Carlo methods (the next topic in your syllabus/slides)
* Specifically, Markov Chain Monte Carlo (MCMC) algorithms like **Gibbs Sampling** (which is introduced immediately after this example on slide 146 [cite: 321]) or Metropolis-Hastings.

So, this example of a non-conjugate Normal model serves primarily to motivate *why* we need to learn about computational approximation techniques in Bayesian inference.