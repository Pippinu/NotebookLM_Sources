**Poisson Likelihood with a Gamma Prior** (Slides 71-74).
This is a common scenario when dealing with **count data**. Imagine you observe $n$ independent counts, $Y_1, Y_2, ..., Y_n$, and you believe they come from a Poisson distribution with an unknown average rate $\theta$. For example, counting the number of emails arriving per hour, the number of accidents at an intersection per month, or the number of radioactive decay events per second.
**1. The Likelihood (Slide 71, Source 140)**
The model assumes each observation $Y_i$ independently follows a Poisson distribution with the same unknown rate parameter $\theta$ ($\theta > 0$):
$Y_i | \theta \sim Poisson(\theta)$
The probability mass function (pmf) for a single Poisson observation is $P(Y_i=y_i|\theta) = \frac{e^{-\theta} \theta^{y_i}}{y_i!}$.
For the entire dataset $y = (y_1, ..., y_n)$, the likelihood function is the product of the individual pmfs:
$L_y(\theta) = \prod_{i=1}^{n} \frac{e^{-\theta} \theta^{y_i}}{y_i!} = \frac{e^{-n\theta} \theta^{\sum_{i=1}^{n} y_i}}{\prod_{i=1}^{n} y_i!}$
For Bayesian inference, we only care about the terms involving $\theta$. Ignoring the constant term $1 / \prod y_i!$, the likelihood kernel is:
$L_y(\theta) \propto e^{-n\theta} \theta^{\sum y_i}$
Notice the functional form: it involves $e$ raised to a term linear in $\theta$, multiplied by $\theta$ raised to some power. The slides note this looks like $e^{-b\theta} \theta^a$.
**2. The Conjugate Prior: Gamma Distribution (Slide 72, Source 142)**
We need a prior distribution $\pi(\theta)$ for the rate parameter $\theta$, which must be positive ($\theta > 0$). We look for a distribution whose functional form "matches" the likelihood kernel $e^{-b\theta} \theta^a$.
The **Gamma distribution** fits this perfectly. The pdf of a Gamma distribution with **shape parameter $s > 0$** and **rate parameter $r > 0$** is:
$\pi(\theta) = \pi_{(s,r)}(\theta) = \frac{r^s}{\Gamma(s)} \theta^{s-1} e^{-r\theta}$, for $\theta > 0$.
The kernel (the part depending on $\theta$) is $\theta^{s-1} e^{-r\theta}$. This matches the form required! Therefore, the Gamma distribution is **conjugate** to the Poisson likelihood.
So, we choose a Gamma prior for $\theta$:
$\theta \sim Gamma(shape=s_{prior}, rate=r_{prior})$
**3. The Posterior Distribution (Slide 72, Source 142)**
Using Bayes' theorem, the posterior is proportional to likelihood times prior:
$\pi(\theta|y) \propto L_y(\theta) \times \pi(\theta)$
$\pi(\theta|y) \propto \left[ e^{-n\theta} \theta^{\sum y_i} \right] \times \left[ \theta^{s_{prior}-1} e^{-r_{prior}\theta} \right]$
Combining terms involving $\theta$:
$\pi(\theta|y) \propto \theta^{(\sum y_i) + s_{prior} - 1} \times e^{-(n + r_{prior})\theta}$
This resulting kernel is exactly the kernel of another Gamma distribution!
The posterior distribution for $\theta$ is:
$\theta | y \sim Gamma(shape=s_{post}, rate=r_{post})$
**4. Simple Updating Rules (Slide 72, Source 142)**
The hyperparameters are updated simply as:
* Posterior Shape: $s_{post} = s_{prior} + \sum_{i=1}^{n} y_i$ (Prior shape + Total count in data)
* Posterior Rate: $r_{post} = r_{prior} + n$ (Prior rate + Number of observations)
You can interpret the prior parameters $s_{prior}$ and $r_{prior}$ as representing prior information equivalent to having observed a total count of $s_{prior}$ over a prior "exposure" of $r_{prior}$. The posterior then simply adds the observed total count ($\sum y_i$) to the prior shape and the new exposure ($n$) to the prior rate.
**5. Reparameterization & Interpretation (Slides 73-74, Sources 143-144)**
Similar to the Beta distribution, we can reparameterize the Gamma prior using:
* Prior Mean: $\mu_{prior} = s_{prior} / r_{prior}$
* Prior "Strength" or pseudo-exposure: $\nu_{prior} = r_{prior}$ (Note: Slide 73 uses $\nu$, let's stick to $r$ to avoid confusion with other uses of $\nu$)
With this, the update rules lead to an intuitive posterior mean:
* Posterior Strength: $r_{post} = r_{prior} + n$
* Posterior Mean: $\mu_{post} = \frac{s_{post}}{r_{post}} = \frac{s_{prior} + \sum y_i}{r_{prior} + n}$
This posterior mean can be rewritten as a **weighted average** of the prior mean and the data's sample mean $\bar{y}_n = (\sum y_i) / n$ (which is also the MLE for the Poisson rate $\theta$):
$\mu_{post} = w \cdot \mu_{prior} + (1-w) \cdot \bar{y}_n$
where the weight is $w = \frac{r_{prior}}{r_{prior} + n}$.
Again, the posterior mean is a blend of the prior mean and the data mean, weighted by the relative strength/exposure of the prior ($r_{prior}$) versus the data ($n$). As $n$ increases, the weight $w$ goes to 0, and the posterior mean is dominated by the data ($\bar{y}_n$).
This Poisson-Gamma example further illustrates the convenience and interpretability of using conjugate priors for standard likelihoods.