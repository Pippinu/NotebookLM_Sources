Okay, let's break down this R exercise which focuses on the Poisson-Gamma conjugate model, including prior elicitation.
First, here is the R code from the file with more explanatory comments added, particularly focusing on the initial elicitation part:
## R Code with Enhanced Comments
```R
# Clear the R environment workspace
rm(list=ls())
# --- Lecture week #03: Poisson Model - Conjugate Bayesian Analysis ---
# Focus: PRIOR ELICITATION within the GAMMA family for a POISSON likelihood
# Model Setup:
# Assume data Y_i are conditionally i.i.d observations from Poisson(theta)
# where theta is the unknown rate parameter (theta > 0).
# We use the conjugate prior for theta: theta ~ Gamma(rate=r, shape=s)
# --- Understanding Basic Gamma Distribution Functions in R ---
# Example parameters for a Gamma distribution
r_example <- 2 # rate parameter
s_example <- 30 # shape parameter
# qgamma: Quantile function (Inverse CDF)
# Calculates the value of theta below which a certain probability mass lies.
qgamma(0.5, shape=s_example, rate=r_example) # Output: Median (0.5 quantile) of Gamma(shape=30, rate=2)
qgamma(0.025, shape=s_example, rate=r_example) # Output: 2.5th percentile
# pgamma: Cumulative Distribution Function (CDF)
# Calculates the probability P(theta <= x)
# Difference gives probability within an interval P(lower < theta <= upper)
pgamma(3.5, shape=s_example, rate=r_example) - pgamma(0.5, shape=s_example, rate=r_example) # Output: P(0.5 < theta <= 3.5)
# dgamma: Density function (PDF)
# Calculates the height of the probability density curve at specific theta values.
# Plotting the Gamma density curve:
curve(dgamma(x, rate=r_example, shape=s_example),
from=0, to=10*sqrt(s_example/r_example^2), # Plot range up to ~10 std devs from mean
xlab=expression(theta), ylab=expression(pi(theta)), # Axis labels
main=paste0("Gamma Prior distribution \n ", # Title
paste0("with rate r=",round(r_example,1)," and shape s=",round(s_example,3))))
# Add point on x-axis showing the 2.5th percentile calculated above
points(qgamma(0.025, shape=s_example, rate=r_example), 0, col="red", pch=16)
# --- Prior Elicitation Goal: Matching Mean and Variance ---
# Our First Goal: Find Gamma hyperparameters (rate=r, shape=s) that match these prior beliefs:
# Belief 1: Prior Mean E[theta] = 3
# Belief 2: Prior Variance Var[theta] = 10
# Formulas relating Gamma parameters to mean and variance:
# E[theta] = shape / rate = s / r
# Var[theta] = shape / rate^2 = s / r^2
# We need to solve this system of 2 equations for the 2 unknowns (r, s):
# 1) s / r = 3
# 2) s / r^2 = 10
# Solving the system:
# From (1), s = 3*r.
# Substitute into (2): (3*r) / r^2 = 10 => 3 / r = 10 => r = 3 / 10 = 0.3
# Substitute r back into s = 3*r: s = 3 * (0.3) = 0.9
# So, the elicited hyperparameters are:
r <- 3/10 # rate
s <- 9/10 # shape
# Let's check if this Gamma(shape=0.9, rate=0.3) distribution has the desired mean and variance.
# Theoretical Mean = s / r = (9/10) / (3/10) = 9/3 = 3 (Matches Belief 1)
# Theoretical Variance = s / r^2 = (9/10) / (3/10)^2 = (9/10) / (9/100) = 10 (Matches Belief 2)
# We can also use simulation to approximately verify (using Law of Large Numbers):
# Generate many random samples from the elicited Gamma distribution
num_sims <- 100000
simulated_thetas <- rgamma(num_sims, shape=s, rate=r)
# Calculate sample mean (should be close to 3)
mean(simulated_thetas)
# Calculate sample variance (should be close to 10)
var(simulated_thetas)
# --- Exploring Gamma Family Interactively with 'manipulate' ---
# (Requires the 'manipulate' package, often used within RStudio)
# Load the manipulate library
library(manipulate)
# Interactive plot 1: Explore Gamma shape by varying rate (r) and shape (s) directly
manipulate(
{ # Code block to execute for each slider change
curve(dgamma(x, rate=r, shape=s), # Plot Gamma density
from=0, to=30, n=1000, # Plot range and resolution
xlab=expression(theta), ylab=expression(pi(theta)), # Labels
main=paste0("Gamma Prior distribution \n ", # Title
paste0("with rate r=",round(r,1)," and shape s=",round(s,3)))) # Dynamic title
# Add legend showing current parameters and calculated mean/variance
legend(x="topright", legend=c(paste0("rate r=", round(r,3)),
paste0("shape s=",round(s,3)),
paste0("mean =",round(s/r,3)),
paste0("var =",round(s/r^2,3))))
},
# Define sliders for r and s
r=slider(min=0.1, max=30, step=0.1, initial=0.3), # Rate slider
s=slider(min=0.1, max=30, step=0.1, initial=0.9) # Shape slider
)
# Interactive plot 2: Similar to above, but adds calculation of P(theta <= 4)
manipulate(
{
curve(dgamma(x, rate=r, shape=s), from=0, to=30, n=2000,
xlab=expression(theta), ylab=expression(pi(theta)),
main=paste0("Gamma Prior distribution \n ",
paste0("with rate r=",round(r,1)," and shape s=",round(s,3))))
# Add probability calculation P(theta <= 4) to the legend
legend(x="topright", legend=c(paste0("rate r=", round(r,3)),
paste0("shape s=",round(s,3)),
paste0("mean=",round(s/r,3)),
paste0("var =",round(s/r^2,3)),
paste0("P(0,4)=",round(pgamma(4, rate=r, shape=s),3)))) # Calculate CDF
},
r=slider(min=0.1, max=30, step=0.1, initial=3), # Example initial values
s=slider(min=0.1, max=30, step=0.1, initial=3)
)
# Suggestion for modification: Could change sliders to control mean (mu_0) and variance (sigma2_0)
# and calculate r, s internally using r = mu_0 / sigma2_0 and s = mu_0^2 / sigma2_0.
# --- Conjugate Bayesian Analysis for Poisson Model on Simulated Data ---
# Our Second Goal: Understand the Bayesian update process (prior -> posterior)
# using simulated data where we know the true generating parameter.
# Simulate synthetic count data Y_i from Poisson(theta_cheat)
# Scenario: Y = number of car accidents in Rome, Thu 15:00-16:00.
# Set the "true" (but unknown in reality) rate parameter for simulation
theta_cheat <- 13
# Plot the true Poisson distribution
plot(0:35, dpois(0:35, lambda=theta_cheat), type="h", main=paste("True Data Generating Dist: Poisson(",theta_cheat,")"))
# Simulate observed data (e.g., n=1 observation)
set.seed(123) # for reproducibility
n <- 1
y_obs <- rpois(n, lambda=theta_cheat)
print(paste("Simulated data (n=", n, "):", y_obs))
# Calculate sufficient statistics from data
sum_y <- sum(y_obs) # Total count
n <- length(y_obs) # Number of observations
# Use the prior elicited earlier (matching mean=3, var=10)
s <- 9/10 # prior shape
r <- 3/10 # prior rate
# Plot the elicited prior distribution again
curve(dgamma(x, shape=s, rate=r), from=0, to=30, n=2000, col="blue", lwd=2, ylim=c(0,0.5),
xlab=expression(theta), ylab="Density", main="Prior vs Posterior")
# Calculate posterior parameters using conjugate update rules:
# s_post = s_prior + sum(y_i)
# r_post = r_prior + n
s_post <- s + sum_y
r_post <- r + n
# Add the posterior Gamma density curve to the plot
curve(dgamma(x, shape=s_post, rate=r_post), from=0, to=30, n=2000, add=TRUE, col="red", lwd=2)
# Compare Prior Mean vs Posterior Mean
prior_mean <- s/r
posterior_mean <- s_post/r_post
print(paste("Prior Mean:", round(prior_mean, 3)))
print(paste("Posterior Mean:", round(posterior_mean, 3)))
# Show MLE (sample mean for Poisson) and posterior mean/median/mode
mle <- mean(y_obs)
abline(v=mle, lty=2, col="green") # MLE (sample mean)
abline(v=posterior_mean, lty=1, col="red") # Posterior Mean
# Posterior median
posterior_median <- qgamma(0.5, shape=s_post, rate=r_post)
abline(v=posterior_median, lty=3, col="purple") # Posterior Median
# Posterior mode (if shape > 1)
if(s_post >= 1){ posterior_mode <- (s_post - 1) / r_post; abline(v=posterior_mode, lty=4, col="orange")} else { posterior_mode <- 0 }
legend("topright", legend=c(paste("Prior (mean=",round(prior_mean,1),")"),
paste("Posterior (mean=",round(posterior_mean,1),")"),
paste("MLE=",round(mle,1)), "Post Median", "Post Mode"),
col=c("blue","red","green","purple","orange"), lty=c(1,1,2,3,4), lwd=2)
# --- Interactive Visualization of Prior -> Posterior Update ---
# Use manipulate to see how prior choice (r, s) affects the posterior update
# given the fixed observed data (y_obs, n)
manipulate(
{ # Code block for manipulate
par(mfrow=c(2,1), mar=c(3,3,2,1)) # Setup 2 plots vertically
# Plot Prior
curve(dgamma(x, rate=r, shape=s), from=0, to=30, n=1000,
xlab="", ylab="", main=paste0("Prior Gamma(r=",round(r,2),", s=",round(s,2),")"), cex.main=1)
legend(x="topright", legend=c(paste0("mean=",round(s/r,2)),
paste0("var =",round(s/r^2,2)),
paste0("P(0,4)=",round(pgamma(4,rate=r,shape=s),2))), bty="n")
abline(v=s/r, col="gray", lty=2) # Prior mean line
# Calculate Posterior parameters
current_s_post <- s + sum(y_obs)
current_r_post <- r + length(y_obs)
current_post_mean <- current_s_post / current_r_post
current_post_var <- current_s_post / current_r_post^2
# Plot Posterior
curve(dgamma(x, rate=current_r_post, shape=current_s_post), from=0, to=30, n=1000,
xlab=expression(theta), ylab="", main=paste0("Posterior Gamma(r=",round(current_r_post,2),", s=",round(current_s_post,2),")"), cex.main=1)
legend(x="topright", legend=c(paste0("mean=",round(current_post_mean,2)),
paste0("var =",round(current_post_var,2)),
paste0("P(0,4)=",round(pgamma(4,rate=current_r_post,shape=current_s_post),2))), bty="n")
abline(v=current_post_mean, col="gray", lty=2) # Posterior mean line
abline(v=mean(y_obs), col="green", lty=3) # Add MLE line for reference
},
# Sliders for prior parameters r and s
r=slider(min=0.1, max=30, step=0.1, initial=0.3), # Prior rate slider
s=slider(min=0.1, max=30, step=0.1, initial=0.9) # Prior shape slider
)
# Suggestion: Re-run the synthetic data part (from line ~60) with a larger sample size 'n'
# (e.g., n=10 or n=50) to see how more data influences the posterior, making it narrower
# and pulling it closer to the true value (theta_cheat=13) and the MLE.
# --- Negative Binomial Predictive Distributions (Poisson-Gamma Case) ---
# Recap: Prior predictive is NegBinom(size=s_prior, prob=r_prior/(r_prior+1))
# Posterior predictive is NegBinom(size=s_post, prob=r_post/(r_post+1))
# Define prior parameters (using elicited values)
r <- 3/10
s <- 9/10
# Define posterior parameters (based on n=1, y_obs)
s_post <- s + sum(y_obs)
r_post <- r + n
# Calculate probability parameter 'p' for NegBinom for prior predictive
p_prior <- r / (r + 1)
# Calculate probability parameter 'p' for NegBinom for posterior predictive
p_post <- r_post / (r_post + 1) # Note: Slides used p = rate/(rate+1) convention
# Define support (possible counts)
support <- 0:35 # Range of interest for number of accidents
# Plot Prior Predictive (Negative Binomial)
plot(support, dnbinom(support, prob=p_prior, size=s), type="h", col="blue",
main="Predictive Distributions", ylab="Probability", xlab="Predicted Count")
points(support, dnbinom(support, prob=p_prior, size=s), type="p", pch=16, col="blue")
# Plot Posterior Predictive (Negative Binomial)
lines(support, dnbinom(support, prob=p_post, size=s_post), type="h", col="orange")
points(support, dnbinom(support, prob=p_post, size=s_post), type="p", pch=16, col="orange")
# Compare with Plug-in Poisson prediction using posterior mean
theta_plugin <- posterior_mean # Use posterior mean as estimate
lines(support, dpois(support, lambda=theta_plugin), type="l", col="green", lty=2) # Plot plug-in Poisson
legend("topright", legend=c("Prior Predictive (NegBinom)",
"Posterior Predictive (NegBinom)",
paste("Plug-in Poisson(mean=",round(theta_plugin,2),")")),
col=c("blue", "orange", "green"), lty=c(1,1,2), pch=c(16,16,NA))
# --- Simulating from Predictive Distributions ---
# Simulate from PRIOR predictive (Two-step: draw theta from prior, then y from Poisson)
theta_sim_prior <- rgamma(n=10000, rate=r, shape=s)
y_sim_prior_pred <- rpois(n=10000, lambda=theta_sim_prior)
# Check simulated distribution vs theoretical prior predictive
# plot(prop.table(table(y_sim_prior_pred)))
# points(support, dnbinom(support, prob=p_prior, size=s), type="p", pch=16, col="red")
# Simulate from POSTERIOR predictive (Two-step: draw theta from posterior, then y from Poisson)
theta_sim_posterior <- rgamma(n=10000, rate=r_post, shape=s_post)
y_sim_posterior_pred <- rpois(n=10000, lambda=theta_sim_posterior)
# Check simulated distribution vs theoretical posterior predictive
# plot(prop.table(table(y_sim_posterior_pred)))
# points(support, dnbinom(support, prob=p_post, size=s_post), type="p", pch=16, col="red")
# Comparing Variances (Illustrating Plug-in Underestimation)
# Theoretical mean of posterior predictive = posterior mean of theta
mean_post_pred_theory <- s_post / r_post
# Theoretical variance of posterior predictive (NegBinom)
var_post_pred_theory <- s_post * (r_post + 1) / (r_post^2)
# Variance of plug-in Poisson prediction = mean of plug-in Poisson
var_plugin <- theta_plugin
print(paste("Posterior Predictive Mean:", round(mean_post_pred_theory, 3)))
print(paste("Posterior Predictive Variance (Theory):", round(var_post_pred_theory, 3)))
print(paste("Plug-in Poisson Variance (=Mean):", round(var_plugin, 3)))
# Note: Post Pred Variance should be > Plug-in Variance, reflecting theta uncertainty.
# --- Conjugate Bayesian Analysis for Poisson Model on Real Data ---
# Application to Students' Awards Data
# Clear workspace again if desired
# rm(list=ls())
# --- Reading and Understanding Data ---
# Load data from CSV file
d_awards <- read.csv(file="students_awards_poisson_model.csv")
# Inspect data structure and first few rows
str(d_awards)
head(d_awards)
# Description of variables:
# num_awards: Outcome (count) - number of awards earned
# math: Predictor (continuous) - math final exam score
# prog: Predictor (categorical) - program type (1=General, 2=Academic, 3=Vocational)
# Data cleaning: Order by ID and make ID a factor
# Check if IDs are sequential
# diff(sort(d_awards$id))
d_awards <- d_awards[order(d_awards$id),]
d_awards$id <- factor(d_awards$id)
# Convert program type 'prog' from integer to factor with meaningful labels
d_awards$prog <- factor(d_awards$prog, levels=1:3, labels=c("General","Academic","Vocational"))
# Check structure again
str(d_awards)
head(d_awards)
# Get summary statistics for all variables
summary(d_awards)
# --- Exploratory Data Analysis (EDA) ---
# Look at the distribution of the outcome variable 'num_awards'
# Overall distribution
f_rel_awards <- prop.table(table(factor(d_awards$num_awards, levels=0:max(d_awards$num_awards)))) # Ensure all levels 0-max are shown
plot(x=names(f_rel_awards), y=f_rel_awards, type="h", ylim=c(0, max(f_rel_awards)*1.1),
main="Distribution of Awards (Overall and by Program)",
xlab="Number of awards", ylab="Proportion")
lines(x=names(f_rel_awards), y=f_rel_awards, type="p", pch=16) # Add points
# Distribution by program type
colors <- c("blue", "red", "green")
for(p in 1:nlevels(d_awards$prog)){
prog_level <- levels(d_awards$prog)[p]
subset_awards <- d_awards$num_awards[d_awards$prog == prog_level]
prog_freq <- prop.table(table(factor(subset_awards, levels=0:max(d_awards$num_awards))))
lines(x=names(prog_freq), y=prog_freq, type="b", col=colors[p], pch=p+1) # Use different colors/symbols
}
legend("topright", legend=c("Overall", levels(d_awards$prog)), pch=c(16, 2:4), col=c("black", colors))
# Calculate descriptive statistics (mean, median, sd) by program type
aggregate(d_awards$num_awards, by=list(Program = d_awards$prog), FUN=mean)
aggregate(d_awards$num_awards, by=list(Program = d_awards$prog), FUN=median)
aggregate(d_awards$num_awards, by=list(Program = d_awards$prog), FUN=sd)
# --- Bayesian Analysis (Simple Model - Ignoring Predictors) ---
# Model the overall number of awards using Poisson(lambda), lambda ~ Gamma(r, s)
# We are estimating a single rate lambda for all students combined.
# Specify Prior Hyperparameters (e.g., a weak/default Gamma(1,1) prior)
# Note: This is different from the elicited prior used earlier.
prior_shape <- 1 # s_prior
prior_rate <- 1 # r_prior
# Prior Mean = 1/1 = 1; Prior Variance = 1/1^2 = 1
# Calculate summaries from the data needed for the update
s_data <- sum(d_awards$num_awards) # Sum of counts (sum y_i)
n_data <- nrow(d_awards) # Number of observations (n)
# --- Prior to Posterior Update using Conjugate Formulas ---
# Calculate posterior hyperparameters
post_shape <- prior_shape + s_data
post_rate <- prior_rate + n_data
# Visualize the prior and posterior distributions
curve(dgamma(x, shape=prior_shape, rate=prior_rate), from=0, to=2, # Adjust xlim as needed
ylim=c(0, 8), # Adjust ylim as needed
lty=2, col="blue", ylab="Density", xlab=expression(lambda), main="Prior vs Posterior for Award Rate")
curve(dgamma(x, shape=post_shape, rate=post_rate), add=TRUE, n=5000, col="orange")
legend("topright", legend=c(paste("Prior Gamma(",prior_shape,",",prior_rate,")"),
paste("Posterior Gamma(",round(post_shape,1),",",round(post_rate,1),")")),
lty=c(2,1), col=c("blue", "orange"), bty="n")
# --- Posterior Inference ---
# Sample Mean (MLE for Poisson rate)
sample_mean <- mean(d_awards$num_awards)
print(paste("Sample Mean (MLE):", round(sample_mean, 3)))
abline(v=sample_mean, lty=3, col="green") # Add MLE line
# Posterior Mean (Point Estimate)
post_mean <- post_shape / post_rate
print(paste("Posterior Mean:", round(post_mean, 3)))
abline(v=post_mean, lty=1, col="orange") # Add posterior mean line
# Verify weighted average interpretation
w <- n_data / (prior_rate + n_data) # Weight of data
post_mean_alt <- (1 - w) * (prior_shape / prior_rate) + w * sample_mean
print(paste("Posterior Mean (Weighted Avg Check):", round(post_mean_alt, 3)))
# Posterior Mode (Point Estimate) (if post_shape >= 1)
if (post_shape >= 1) {
post_mode <- (post_shape - 1) / post_rate
print(paste("Posterior Mode:", round(post_mode, 3)))
abline(v=post_mode, lty=4, col="purple")
} else { print("Posterior Mode is 0") }
legend("topleft", legend=c("MLE", "Post Mean", "Post Mode"), col=c("green", "orange", "purple"), lty=c(3,1,4), bty="n")
# 95% Equal-Tailed Credible Interval
ET_interval <- qgamma(p=c(0.025, 0.975), shape=post_shape, rate=post_rate)
print(paste("95% ET Credible Interval:", "[", round(ET_interval[1], 3), ",", round(ET_interval[2], 3), "]"))
# Add interval bounds to plot
# abline(v=ET_interval, col="gray")
# 95% HPD Credible Interval
# library(TeachingDemos)
HPD_interval <- TeachingDemos::hpd(qgamma, shape=post_shape, rate=post_rate, conf=0.95)
print(paste("95% HPD Credible Interval:", "[", round(HPD_interval[1], 3), ",", round(HPD_interval[2], 3), "]"))
# Add interval bounds to plot
# abline(v=HPD_interval, col="black")
# Why are ET and HPD similar here?
# The posterior Gamma distribution is reasonably symmetric (check plot), especially with large shape parameter.
print(paste("Posterior shape:", round(post_shape, 1)))
# Hypothesis Testing Example
# Test H0: lambda >= 0.5 vs H1: lambda < 0.5
# Calculate posterior probability of H0
post_prob_H0 <- 1 - pgamma(q=0.5, shape=post_shape, rate=post_rate)
print(paste("Posterior P(lambda >= 0.5):", round(post_prob_H0, 4)))
# Calculate prior probability of H0
prior_prob_H0 <- 1 - pgamma(q=0.5, shape=prior_shape, rate=prior_rate)
print(paste("Prior P(lambda >= 0.5):", round(prior_prob_H0, 4)))
# Bayes Factor calculation
post_odds_H0 <- post_prob_H0 / (1 - post_prob_H0)
prior_odds_H0 <- prior_prob_H0 / (1 - prior_prob_H0)
# Ensure prior odds are not zero or infinity before dividing
if (prior_prob_H0 > 0 && prior_prob_H0 < 1) {
BFactorH0 <- post_odds_H0 / prior_odds_H0
print(paste("Bayes Factor BF(H0 vs H1):", round(BFactorH0, 3)))
} else {
print("Cannot calculate Bayes Factor due to prior probability being 0 or 1.")
}
```
## Explanation of the Exercise
This R script demonstrates several aspects of Bayesian analysis using the **Poisson likelihood** and its conjugate **Gamma prior**.
**Part 1: Prior Elicitation Based on Mean and Variance**
* **Goal:** To find the specific parameters (rate $r$, shape $s$) of a Gamma distribution that match desired prior beliefs about the unknown Poisson rate $\theta$.
* **Beliefs:** The prior mean $E[\theta]$ should be 3, and the prior variance $Var[\theta]$ should be 10.
* **Method:** It uses the formulas $E[\theta] = s/r$ and $Var[\theta] = s/r^2$. By setting these equal to 3 and 10 respectively, it solves the system of two equations to find the required hyperparameters: $r=0.3$ and $s=0.9$.
* **Verification:** It checks this result both theoretically and approximately using simulation (`rgamma`).
* **Exploration:** It includes interactive plots (`manipulate`) to help visualize how changing the rate ($r$) and shape ($s$) parameters affects the Gamma distribution's shape, mean, variance, and probability content in specific intervals.
**Part 2: Bayesian Analysis on Synthetic Data**
* **Goal:** To illustrate the prior-to-posterior update process using data generated from a known Poisson distribution ($Poisson(\theta_{cheat}=13)$).
* **Process:**
* Simulates a small dataset (`n=1` initially) from $Poisson(13)$.
* Uses the *elicited* prior ($Gamma(shape=0.9, rate=0.3)$) from Part 1.
* Calculates the posterior parameters ($s_{post} = s_{prior} + \sum y_i$, $r_{post} = r_{prior} + n$) using the conjugate update rule.
* Visualizes the elicited prior and the resulting posterior distribution, showing how the data updates the belief. Compares prior mean, posterior mean, and the data's MLE ($\bar{y}$).
* Includes an interactive plot (`manipulate`) allowing exploration of how different *prior* choices ($r, s$) would be updated to different posteriors by the *same* fixed synthetic data.
**Part 3: Predictive Distributions (Negative Binomial)**
* **Goal:** To demonstrate the calculation and simulation of the prior and posterior predictive distributions for the Poisson-Gamma model.
* **Method:**
* It correctly identifies both predictive distributions as **Negative Binomial**.
* It calculates the parameters for both the prior predictive (using prior $r, s$) and posterior predictive (using posterior $r_{post}, s_{post}$).
* It plots the theoretical probability mass functions.
* It demonstrates the **two-step simulation** process for both predictive distributions (draw $\theta$ from prior/posterior Gamma, then draw $Y$ from Poisson($\theta$)).
* It compares the variance of the posterior predictive distribution to the variance of a naive "plug-in" Poisson prediction (using the posterior mean as a fixed rate), visually showing that the Bayesian predictive distribution captures more uncertainty.
**Part 4: Application to Real Data (Student Awards)**
* **Goal:** To apply the Poisson-Gamma conjugate model to a real dataset (`students_awards_poisson_model.csv` [cite: 14]).
* **Process:**
* Loads and explores the data (number of awards earned by students).
* **Specifies a simple Gamma(1,1) prior** for the overall average award rate $\lambda$ (note: *different* from the elicited prior in Part 1, likely chosen as a less informative default for this analysis).
* Calculates the posterior parameters using the conjugate update rule based on the sum of awards and the number of students.
* Visualizes the Gamma(1,1) prior and the resulting posterior.
* Calculates posterior summaries: point estimates (mean, mode) and interval estimates (95% ET and HPD credible intervals).
* Performs a hypothesis test ($H_0: \lambda \ge 0.5$) using the posterior probability and calculates the corresponding Bayes Factor.
In essence, the script covers prior elicitation for the Gamma distribution, demonstrates the conjugate Poisson-Gamma update with both synthetic and real data, calculates relevant posterior summaries and tests, and explores the associated predictive distributions.